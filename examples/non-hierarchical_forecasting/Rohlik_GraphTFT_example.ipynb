{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36a37455",
   "metadata": {},
   "source": [
    "## Rohlik Sales Forecasting Challenge\n",
    "\n",
    "**This example demonstrates pytorch-graphstam library's usage for a typical, non-hierarchical multistep forecasting problem**\n",
    "\n",
    "- The solo model achieves a score of ~18.33 i.e. ~top-20 on private leaderboard\n",
    "- An ensemble of this model with xgboost achieves a score of ~17.3 i.e. 4th place on private leaderboard\n",
    "\n",
    "**Competition Overview**  \n",
    "The Rohlik Sales Forecasting Challenge on Kaggle is centered around predicting daily sales for a leading European online grocery retailer, Rohlik. Participants are given historical sales data for numerous products and are tasked with forecasting future sales. \n",
    "\n",
    "**Competition Links**  \n",
    "- [Rohlik Sales Forecasting Challenge (v2) Main Page](https://www.kaggle.com/competitions/rohlik-sales-forecasting-challenge-v2/overview)  \n",
    "- [Data Overview & Download](https://www.kaggle.com/competitions/rohlik-sales-forecasting-challenge-v2/data)  \n",
    "- [Evaluation Details](https://www.kaggle.com/competitions/rohlik-sales-forecasting-challenge-v2/overview/evaluation)  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b1eab3",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "#### install pytorch >= v2.5.x with CUDA version 12.1, also works with with cu118/cu124/cu128 \n",
    "pip install torch==2.5.1 --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "#### install pytorch-geometric >= v2.6.x\n",
    "check version compatibility with torch & os here: https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html\n",
    "pip install torch_geometric\n",
    "\n",
    "#### install pytorch-geometric dependencies for torch v2.5.* & CUDA v12.1\n",
    "pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.5.0+cu121.html\n",
    "    \n",
    "#### install pytorch-graphstam\n",
    "pip install -U git+https://github.com/rsscml/pytorch-graphstam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97879401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# core library imports\n",
    "\n",
    "from pytorch_graphstam import graphstam\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.max_rows\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8a56f5",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e657355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the datafiles -- assuming they are in the current directory\n",
    "train_sales_df = pd.read_csv(\"sales_train.csv\", parse_dates=['date'])\n",
    "test_sales_df = pd.read_csv(\"sales_test.csv\", parse_dates=['date'])\n",
    "inventory_df = pd.read_csv(\"inventory.csv\")\n",
    "calendar_df = pd.read_csv(\"calendar.csv\", parse_dates=['date'])\n",
    "weights_df = pd.read_csv(\"test_weights.csv\")\n",
    "\n",
    "# merge the above datasets into a single dataset\n",
    "# add 'availability' col to test_Sales & set it to null\n",
    "\n",
    "test_sales_df['availability'] = np.nan\n",
    "sales_df = pd.concat([train_sales_df, test_sales_df], axis=0)\n",
    "sales_df = sales_df.sort_values(by=['unique_id','date'], ascending=True).reset_index(drop=True)\n",
    "# merge inventory\n",
    "sales_df = sales_df.merge(inventory_df, on=['unique_id','warehouse'], how='left')\n",
    "# merge calendar\n",
    "calendar_df = pd.get_dummies(calendar_df, columns=['holiday_name'])\n",
    "sales_df = sales_df.merge(calendar_df, on=['date','warehouse'], how='left')\n",
    "# merge weights\n",
    "sales_df = sales_df.merge(weights_df, on=['unique_id'], how='left')\n",
    "\n",
    "# create date features\n",
    "sales_df['weekday'] = sales_df['date'].dt.weekday\n",
    "sales_df['week'] = sales_df['date'].dt.isocalendar().week\n",
    "sales_df['month'] = sales_df['date'].dt.month\n",
    "sales_df['year'] = sales_df['date'].dt.year\n",
    "sales_df['prev_year'] = (sales_df['date'] - timedelta(days=1)).dt.year\n",
    "sales_df['day'] = sales_df['date'].dt.day\n",
    "sales_df['is_month_start'] = sales_df['date'].dt.is_month_start\n",
    "sales_df['is_month_end'] = sales_df['date'].dt.is_month_end\n",
    "sales_df['quarter'] = sales_df['date'].dt.quarter\n",
    "sales_df['weekend']=(sales_df['weekday']>4).astype(np.int8)\n",
    "sales_df['days_since_2020'] = (sales_df['date'] - pd.to_datetime('2020-01-01')).dt.days.astype('int')\n",
    "\n",
    "# add 'country' as a feature\n",
    "store2country = {\n",
    "        'Budapest_1': 'Hungary',\n",
    "        'Prague_2': 'Czechia',\n",
    "        'Brno_1': 'Czechia',\n",
    "        'Prague_1': 'Czechia',\n",
    "        'Prague_3': 'Czechia',\n",
    "        'Munich_1': 'Germany',\n",
    "        'Frankfurt_1': 'Germany'\n",
    "}\n",
    "\n",
    "sales_df['country'] = sales_df['warehouse'].apply(lambda x:store2country[x])\n",
    "\n",
    "# Basic fillna ['total_orders','sales','availability'] as mean of date\n",
    "sales_df['sales'] = sales_df['sales'].fillna(sales_df.groupby(['unique_id','weekday'])['sales'].transform('mean'))\n",
    "sales_df['total_orders'] = sales_df['total_orders'].fillna(sales_df.groupby(['unique_id','weekday'])['total_orders'].transform('mean'))\n",
    "sales_df['availability'] = sales_df['availability'].fillna(sales_df.groupby(['unique_id','weekday'])['availability'].transform('mean'))\n",
    "# fill 0 for any edge cases\n",
    "sales_df['sales'] = sales_df['sales'].fillna(0)\n",
    "sales_df['availability'] = sales_df['availability'].fillna(0)\n",
    "\n",
    "# check for nulls in any columns\n",
    "print(sales_df.columns[sales_df.isnull().any()])\n",
    "\n",
    "# writeout the processed dataset\n",
    "sales_df.to_csv(\"sales_processed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f054f0a7",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0059be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the processed dataset as a pandas dataframe\n",
    "df = pd.read_csv(\"sales_processed.csv\", dtype={'unique_id':'str', 'date':'str'})\n",
    "\n",
    "# optional step: restrict dataset to >= 2021 as prior data is patchy\n",
    "df = df[df['year']>=2021]\n",
    "\n",
    "# optional step: power transform target col using square root\n",
    "df['sales'] = np.sqrt(df['sales'])\n",
    "\n",
    "# the submission requires forecasts for only a subset of unique_ids; extract these ids for re-weighting purpose later\n",
    "solution_df = pd.read_csv(\"solution.csv\")\n",
    "solution_df['unique_id'] = solution_df['id'].str.split('_').str[0]\n",
    "all_solution_unique_ids = solution_df['unique_id'].unique().tolist()\n",
    "\n",
    "print(len(all_solution_unique_ids)) # show show 3625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8725f7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create edge basis columns i.e., these columns can be used to connect product nodes through edges on the basis of some similarity\n",
    "\n",
    "df['common_name'] = df['name'].str.split('_').str[0]\n",
    "df['warehouse_common_name'] = df['warehouse'] + '_' + df['common_name']\n",
    "df['warehouse_name'] = df['warehouse'] + '_' + df['name']\n",
    "df['warehouse_L3_category_name_en'] = df['warehouse'] + '_' + df['L3_category_name_en']\n",
    "df['warehouse_L4_category_name_en'] = df['warehouse'] + '_' + df['L4_category_name_en']\n",
    "df['warehouse_L2_category_name_en'] = df['warehouse'] + '_' + df['L2_category_name_en']\n",
    "df['warehouse_L1_category_name_en'] = df['warehouse'] + '_' + df['L1_category_name_en']\n",
    "df['warehouse_L4_L3_category_name_en'] = df['warehouse'] + '_' + df['L4_category_name_en']+ '_' + df['L3_category_name_en']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46f7118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify all the columns in the dataset we plan to use \n",
    "\n",
    "id_col = 'unique_id'\n",
    "datetime_col = 'date'\n",
    "target_col = 'sales'\n",
    "weight_col = 'weight'\n",
    "date_features = ['weekday','week','month','day','is_month_start','is_month_end','weekend']\n",
    "other_features = ['total_orders', 'sell_price_main']\n",
    "promo_features = ['type_0_discount',\n",
    "                  'type_1_discount',\n",
    "                  'type_2_discount',\n",
    "                  'type_3_discount',\n",
    "                  'type_4_discount',\n",
    "                  'type_5_discount',\n",
    "                  'type_6_discount']\n",
    "\n",
    "\n",
    "event_features = ['holiday',\n",
    "                 'shops_closed', \n",
    "                 'winter_school_holidays',\n",
    "                 'school_holidays',\n",
    "                 'holiday_name_1st Christmas Day',\n",
    "                 'holiday_name_2nd Christmas Day',\n",
    "                 'holiday_name_All Saints Day',\n",
    "                 \"holiday_name_All Saints' Day Holiday\",\n",
    "                 'holiday_name_Ascension day',\n",
    "                 'holiday_name_Assumption of the Virgin Mary',\n",
    "                 'holiday_name_Christmas Eve',\n",
    "                 'holiday_name_Christmas Holiday',\n",
    "                 'holiday_name_Corpus Christi',\n",
    "                 'holiday_name_Cyrila a Metodej',\n",
    "                 'holiday_name_Day of National Unity',\n",
    "                 'holiday_name_Den boje za svobodu a demokracii',\n",
    "                 'holiday_name_Den ceske statnosti',\n",
    "                 'holiday_name_Den osvobozeni',\n",
    "                 'holiday_name_Den vzniku samostatneho ceskoslovenskeho statu',\n",
    "                 'holiday_name_Easter Monday',\n",
    "                 'holiday_name_Epiphany',\n",
    "                 'holiday_name_German Unity Day',\n",
    "                 'holiday_name_Good Friday',\n",
    "                 'holiday_name_Hungary National Day Holiday',\n",
    "                 'holiday_name_Independent Hungary Day',\n",
    "                 'holiday_name_Jan Hus',\n",
    "                 'holiday_name_Labour Day',\n",
    "                 'holiday_name_Memorial Day of the Republic',\n",
    "                 'holiday_name_Memorial day of the 1956 Revolution',\n",
    "                 'holiday_name_National Defense Day',\n",
    "                 'holiday_name_New Years Day',\n",
    "                 'holiday_name_Reformation Day',\n",
    "                 'holiday_name_State Foundation Day',\n",
    "                 'holiday_name_Whit monday',\n",
    "                 'holiday_name_Whit sunday']\n",
    "                  \n",
    "global_context_cols = ['country',\n",
    "                       'warehouse',\n",
    "                       'name',\n",
    "                       'L1_category_name_en',\n",
    "                       'L2_category_name_en', \n",
    "                       'L3_category_name_en',\n",
    "                       'L4_category_name_en']\n",
    "\n",
    "static_context_cols = ['warehouse_L2_category_name_en', 'warehouse_L3_category_name_en','warehouse_common_name']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b84abf0",
   "metadata": {},
   "source": [
    "#### A bit about global_context_cols & static_context_cols\n",
    "\n",
    "- global_context_cols: This is a list of static, categorical columns which are to be used as 'context' for a forecast key. The context is provided to a key node through an in-coming directed edge. In the present case, we want every key node to have a context comprising of the country & warehouse to which it belongs, various L1/L2/L3 & L4 categories under which it comes & the product name. \n",
    "\n",
    "- static_context_cols: This is a list of static, categorical columns which are to be used as the basis for drawing point to point bidirectional edges between key nodes. In the present case, we want to connect all the key nodes that are common to a warehouse & the L2 category in that warehouse, so, the basis for inferring such edges is the column: 'warehouse_L2_category_name_en'. Similarily, we connect all the key nodes common to a warehouse & the L3 category using 'warehouse_L3_category_name_en' & all the key nodes common to a warehouse & having a common product name are connected using 'warehouse_common_name'.\n",
    "\n",
    "Note 1: The two lists are mutually exclusive, it may require some trial to figure out which columns should be in which list.\n",
    "\n",
    "Note 2: Since columns in the 'static_context_cols' list form the basis for p2p edges between the key nodes, it has the biggest impact on size of the graph & speed of computation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7402f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a helper function used to determine the non-default weights for various forecast_keys\n",
    "\n",
    "def create_weights(df):\n",
    "    max_test_wt = df[df['unique_id'].isin(all_solution_unique_ids)]['weight'].unique().max()\n",
    "    q25_test_wt = np.quantile(df['weight'].unique(), q=0.25)\n",
    "    df['weight'] = np.clip(df['weight'], a_min=q25_test_wt, a_max=max_test_wt)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2debed5b",
   "metadata": {},
   "source": [
    "#### Define config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0acf930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test cutoffs\n",
    "train_till = \"2024-04-30\"\n",
    "test_till = \"2024-06-02\"\n",
    "\n",
    "# infer cutoffs\n",
    "infer_start = \"2024-06-03\"\n",
    "infer_end = \"2024-06-16\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5514e09",
   "metadata": {},
   "source": [
    "The following config is meant to be used for the **GraphTFT** model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9314ab5f",
   "metadata": {},
   "source": [
    "To view complete config details, run the following:\n",
    "- graphstam.show_config_template()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342c955a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build config for GraphTFT model\n",
    "\n",
    "\n",
    "# base features\n",
    "features_config = {\n",
    "                    \"id_col\": id_col,\n",
    "                    \"target_col\": target_col,      \n",
    "                    \"time_index_col\": datetime_col,      \n",
    "                    \"global_context_col_list\": global_context_cols,   \n",
    "                    \"static_cat_col_list\": static_context_cols,       \n",
    "                    \"temporal_known_num_col_list\": event_features + other_features + date_features + promo_features,  \n",
    "                    \"temporal_unknown_num_col_list\": [],  \n",
    "                    \"wt_col\": weight_col\n",
    "}\n",
    "\n",
    "# Optional: \n",
    "# rolling features: use moving averages & std.dev over various window sizes of 7,14 & 28 days.\n",
    "\n",
    "rolling_features =  [(datetime_col,'mean', 7, 0),\n",
    "                     (datetime_col,'mean', 14, 0),\n",
    "                     (datetime_col,'std', 14, 0),\n",
    "                     (datetime_col,'mean', 28, 0),\n",
    "                     (datetime_col,'std', 28, 0)]\n",
    "\n",
    "# data config\n",
    "data_config = {\n",
    "                \"max_lags\": 60,        # max lookback period\n",
    "                \"fh\": 14,              # forecast horizon\n",
    "                \"train_till\": train_till, \n",
    "                \"test_till\": test_till,\n",
    "                \"scaling_method\": 'mean_scaling', # scales target using the mean scaling method\n",
    "                \"tweedie_out\": False,             # useful in cases where the target variable is tweedie or poisson distributed  \n",
    "                \"tweedie_variance_power\": [1.1],  # the hyperparameter for tweedie distribution\n",
    "                \"interleave\": 1,                  # if == 1, trains on all samples in the dataset, if 2, trains on half the samples & so on \n",
    "                \"recency_weights\": False,         # gives greater importance more recent samples\n",
    "                \"recency_alpha\": 1,               # controls the skew of recency_weights, greater this value more heavily the recent samples are weighted\n",
    "}\n",
    "\n",
    "\n",
    "# model config\n",
    "model_config = {\n",
    "                \"model_dim\": 64,                  # defines model dimensionality & determines the no. of parameters in the model\n",
    "                \"num_gnn_layers\": 1,              # no. of gnn layers to use (in the encoder side of TFT) \n",
    "                \"num_attn_layers\": 4,             # no. of attention layers to use. 2-4 should be sufficient for most cases. \n",
    "                \"num_rnn_layers\": 1,              # the no. of rnn layers to use (this is again a component of the TFT architecture)\n",
    "                \"heads\": 1,                       # no. of heads for attention \n",
    "                \"dropout\": 0,                     # dropout regularization\n",
    "                \"device\": 'cuda',                 # 'cuda': run on GPU, else, 'cpu' -- not practical!\n",
    "                \"gbt\": False,                     # if True, fits a booster model to the residuals of the base model i.e., trains > 1 model (given by n_boosters)\n",
    "                \"n_boosters\": 1,                  # if gbt == True, no. of booster models to use; > 1 is impractical in most cases\n",
    "}\n",
    "\n",
    "\n",
    "# train config\n",
    "# Scheduler params - default works well - for more information check pytorch documentation\n",
    "\n",
    "scheduler_params_config = {\n",
    "                            \"factor\": 0.5, \n",
    "                            \"patience\": 5, \n",
    "                            \"threshold\": 0.0001, \n",
    "                            \"min_lr\": 0.0001, \n",
    "                            \"clip_gradients\": False, \n",
    "                            \"max_norm\": 2.0, \n",
    "                            \"norm_type\": 2\n",
    "} \n",
    "\n",
    "\n",
    "train_config = {\n",
    "                \"lr\": 0.001,                       # learning rate  \n",
    "                \"min_epochs\": 10,                  # minimum passes over the training dataset\n",
    "                \"max_epochs\": 100,                 # max. epochs\n",
    "                \"patience\": 5,                     # no. of epochs to wait for improvement in loss   \n",
    "                \"model_prefix\": \"rohlik\",          # used to determine the name of the saved models\n",
    "                \"loss_type\": 'Huber',              # loss fn. other options are: ['RMSE','Quantile','Tweedie']\n",
    "                \"huber_delta\": 0.5,                # hyperparam for the huber loss fn.\n",
    "                \"use_amp\": True,                   # autocast float32 to float16 to save memory at the expense of accuracy  \n",
    "                \"use_lr_scheduler\": True,          # whether to use decreasing learning rate schedule \n",
    "                \"scheduler_params\": scheduler_params_config, # params for the lr scehduler\n",
    "                \"sample_weights\": True             # whether to use sample weighted loss for minimization  \n",
    "}    \n",
    "\n",
    "\n",
    "# infer config\n",
    "infer_config = {\n",
    "                \"infer_start\": infer_start,\n",
    "                \"infer_end\": infer_end,\n",
    "}\n",
    "\n",
    "\n",
    "# final config\n",
    "config = {\n",
    "            \"model_type\": 'GraphTFT',              # other options: ['GraphSeq2Seq', 'SimpleGraph']\n",
    "            \"working_dir\": './lib_tft_dir',        # Save models & logs here\n",
    "            \"features_config\": features_config,\n",
    "            \"rolling_features\": rolling_features,\n",
    "            \"data_config\": data_config,\n",
    "            \"model_config\": model_config,\n",
    "            \"train_config\": train_config,\n",
    "            \"infer_config\": infer_config\n",
    "}\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8975f6c2",
   "metadata": {},
   "source": [
    "#### Train & Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc23a310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train: one model per country\n",
    "\n",
    "for country in df['country'].unique().tolist():\n",
    "    print(\"\\ntraining model for country: \", country)\n",
    "    df_country = df[df['country']==country]\n",
    "    df_country = create_weights(df_country)\n",
    "    print(\"no. of forecast keys: \", df_country['unique_id'].nunique())\n",
    "    \n",
    "    # create main 'gml' object\n",
    "    gmlobj = graphstam.gml(config)\n",
    "    \n",
    "    # build model & initialize\n",
    "    gmlobj.build(df_country)\n",
    "    \n",
    "    # train\n",
    "    gmlobj.train()\n",
    "    \n",
    "    # infer - saves output as csv at: forecast_filepath \n",
    "    forecast_df = gmlobj.infer(forecast_filepath=f\"./output/rohlik_tft_{country}.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dbf171",
   "metadata": {},
   "source": [
    "#### Post-process for submission -- optional step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4227d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge country forecast files\n",
    "df_hungary = pd.read_csv(\"./output/rohlik_tft_Hungary.csv\")\n",
    "df_germany = pd.read_csv(\"./output/rohlik_tft_Germany.csv\")\n",
    "df_czechia = pd.read_csv(\"./output/rohlik_tft_Czechia.csv\")\n",
    "\n",
    "out_df = pd.concat([df_hungary, df_germany, df_czechia], axis=0)\n",
    "\n",
    "# merge with main df\n",
    "out_df = out_df.merge(sales_df, on=['name','warehouse'], how='left')\n",
    "\n",
    "# format as required\n",
    "out_df['id'] = out_df['unique_id_'].astype(str) + '_' + out_df['date'].astype(str)\n",
    "out_df = out_df[['id','forecast']]\n",
    "\n",
    "# merge with submission file\n",
    "submit_df = pd.read_csv(\"solution.csv\")\n",
    "submit_df = submit_df.merge(out_df, on=['id'], how='inner')\n",
    "submit_df = submit_df.drop(columns=['sales_hat']).rename(columns={'forecast':'sales_hat'})\n",
    "# clip min. at 0\n",
    "submit_df['sales_hat'] = np.where(submit_df['sales_hat']<0, 0, submit_df['sales_hat'])\n",
    "# undo power transformation\n",
    "submit_df['sales_hat'] = np.power(submit_df['sales_hat'], 2)\n",
    "\n",
    "# final submission file\n",
    "submit_df.to_csv(\"sample_submission_gtft.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
